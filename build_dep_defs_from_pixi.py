#!/usr/bin/env python3

import subprocess
import yaml
import json

# Step 1: Export the environment using pixi
result = subprocess.run(
    ["pixi", "run", "conda", "env", "export", "--no-builds"],
    capture_output=True, text=True, check=True
)

# Step 2: Parse the YAML
env_data = yaml.safe_load(result.stdout)

# Step 3: Clean up - remove pip and prefix
if "dependencies" in env_data:
    cleaned_deps = []
    for dep in env_data["dependencies"]:
        if isinstance(dep, dict) and "pip" in dep:
            continue  # skip pip section
        cleaned_deps.append(dep)
    env_data["dependencies"] = cleaned_deps

env_data.pop("prefix", None)  # remove prefix
env_data.pop("name", None)  # remove name

# channels only conda-forge and bioconda
if "channels" in env_data:
    env_data["channels"] = [ch for ch in env_data["channels"] if ch in ["conda-forge", "bioconda"]]
else:
    raise ValueError("No channels found in the environment data.")

# Step 4: Write to environment.yml
with open("environment.yml", "w") as f:
    f.write("# This file was autogenerated by build_dep_defs_from_pixi.py - do not change manually.\n")
    yaml.dump(env_data, f, default_flow_style=False)

print("Clean environment.yml written.")

# Write requirements.txt via 'pip list --format json'
result = subprocess.run(
    ["pixi", "run", "pip", "list", "--format", "json"],
    capture_output=True, text=True, check=True
)
# Parse the JSON output
pip_list = json.loads(result.stdout)
# Write to requirements.txt
with open("requirements.txt", "w") as f:
    f.write("# This file was autogenerated by build_dep_defs_from_pixi.py - do not change manually.\n")
    for package in pip_list:
        line = f"{package['name']}=={package['version']}\n"
        f.write(line)
print("requirements.txt written.")
